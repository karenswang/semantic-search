{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='ISO-8859-1') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        current_record = ''\n",
    "        for line in infile:\n",
    "            if line.count('|') == 10:  # Identifies a new record\n",
    "                if current_record:  # If there's an ongoing record, write it before starting a new one\n",
    "                    outfile.write(current_record + '\\n')\n",
    "                current_record = line.rstrip('\\n')  # Start a new record\n",
    "            else:\n",
    "                # Part of the ongoing BillText; append including a space to avoid word merging\n",
    "                current_record += ' ' + line.rstrip('\\n')\n",
    "        # Write the last record if it exists\n",
    "        if current_record:\n",
    "            outfile.write(current_record)\n",
    "\n",
    "# Example usage\n",
    "input_path = \"../data_storage/legislation/complete_full_text.txt\"\n",
    "output_path = \"../data_storage/legislation/preprocessed_full_text.txt\"\n",
    "preprocess_file(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Parse Legislation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# File path to the preprocessed file\n",
    "file_path = \"../data_storage/legislation/preprocessed_full_text.txt\"\n",
    "\n",
    "schema = \"BillID STRING, StateCode STRING, StateBillID STRING, ShortBillName STRING, Created STRING, SponsorParty STRING, billtype STRING, status STRING, CommitteeCategories STRING, statesummary STRING, BillText STRING\"\n",
    "column_names = ['BillID', 'StateCode', 'StateBillID', 'ShortBillName', 'Created', 'SponsorParty', 'billtype', 'status', 'CommitteeCategories', 'statesummary', 'BillText']\n",
    "\n",
    "# Load data\n",
    "data = spark.read.option(\"delimiter\", \"|\").csv(file_path, schema=schema)\n",
    "\n",
    "# Combine remaining columns into a single column if there are more than 11 columns\n",
    "remaining_columns = data.columns[10:]\n",
    "data = data.withColumn(\"BillText\", concat_ws(\"|\", *[data[col] for col in remaining_columns]))\n",
    "\n",
    "# Select only the first 11 columns\n",
    "data = data.select(column_names)\n",
    "\n",
    "data.show()\n",
    "\n",
    "csv_file_path = \"../data_storage/legislation/complete_full_text.csv\"\n",
    "data.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_file_path)\n",
    "\n",
    "\n",
    "\n",
    "# csv_directory_path = \"../data_storage/legislation/complete_full_text.csv\"\n",
    "\n",
    "# Load all part-files from the directory into a DataFrame\n",
    "# df = spark.read.csv(path=csv_directory_path, header=True, inferSchema=True)\n",
    "# df.toPandas().to_csv(\"../data_storage/legislation/complete_full_text_pdf.csv\", index=False)\n",
    "\n",
    "# # Stop Spark session\n",
    "# spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/08 19:09:10 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 21) 1]\n",
      "java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "24/04/08 19:09:10 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 21) (192.168.0.82 executor driver): java.net.SocketException: Connection reset\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\n",
      "\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n",
      "\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n",
      "\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n",
      "\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "24/04/08 19:09:10 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o58.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 21) (192.168.0.82 executor driver): java.net.SocketException: Connection reset\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.net.SocketException: Connection reset\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/LLM-publication-patterns-public/lib/python3.11/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/LLM-publication-patterns-public/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.virtualenvs/LLM-publication-patterns-public/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.virtualenvs/LLM-publication-patterns-public/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o58.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 21) (192.168.0.82 executor driver): java.net.SocketException: Connection reset\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.net.SocketException: Connection reset\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "import weaviate\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "weaviate_api_key = os.getenv('WEAVIATE_API_KEY')\n",
    "weaviate_url = os.getenv('WEAVIATE_URL')\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "billtrack50_api_key = os.getenv('bill_tracker_api_key')\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url = weaviate_url,\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=weaviate_api_key), \n",
    "    additional_headers = {\n",
    "        \"X-OpenAI-Api-Key\": openai_key\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "if client.schema.exists(\"Legislation\"):\n",
    "    client.schema.delete_class(\"Legislation\")\n",
    "    \n",
    "# for pre-vectorized data\n",
    "class_obj = {\n",
    "    \"class\": \"Legislation\",\n",
    "    \"vectorizer\": \"none\",\n",
    "    \"moduleConfig\": {\n",
    "        \"generative-openai\": {}  # Ensure the `generative-openai` module is used for generative queries\n",
    "    }\n",
    "}\n",
    "\n",
    "client.schema.create_class(class_obj)\n",
    "\n",
    "\n",
    "# for pre-vectorized data        \n",
    "client.batch.configure(batch_size=100)\n",
    "with client.batch as batch:\n",
    "    for index, row in df.iterrows():\n",
    "        properties = {\n",
    "            \"billID\": row['billID'],\n",
    "            \"stateBillID\": row['stateBillID'],\n",
    "            \"stateCode\": row['stateCode'],            \n",
    "            \"billName\": row['billName'],\n",
    "            \"summary\": row['summary'],\n",
    "            \"sponsorCount\": row['sponsorCount'],\n",
    "            \"sponsors\": row['sponsors'],\n",
    "            \"subjects\": row['subjects'],\n",
    "            \"keyWords\": row['keyWords'],\n",
    "            \"actions\": row['actions'],\t\n",
    "            \"lastAction\": row['lastAction'],\n",
    "            \"actionDate\": row['actionDate'],\n",
    "            \"votes\": row['votes'],\n",
    "            \"billProgress\": row['billProgress'],\t\n",
    "            \"officialDocument\": row['officialDocument'],\n",
    "            \"created\": row['created']   \n",
    "        }\n",
    "        # concatenated_vector = row['billName_vector'] + row['summary_vector'] + row['keyWords_vector']\n",
    "\n",
    "        # Adjust the property names and structure according to your schema requirements\n",
    "        batch.add_data_object(properties, \"Legislation\", vector=row['summary_vector'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path and get all CSV files\n",
    "csv_file_path = \"../data_storage/legislation/complete_full_text.csv/*.csv\"\n",
    "csv_files = glob.glob(csv_file_path)\n",
    "\n",
    "# Define your column names here based on your dataset's requirements\n",
    "column_names = ['BillID', 'StateCode', 'StateBillID', 'ShortBillName', 'Created', 'SponsorParty', 'billtype', 'status', 'CommitteeCategories', 'statesummary', 'BillText']\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in csv_files[]:\n",
    "    processed_rows = []\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            # Split the line into fields\n",
    "            fields = line.strip().split(',')\n",
    "\n",
    "            # Handle extra fields for the 4th column (ShortBillName)\n",
    "            if len(fields) > 4:\n",
    "                # Combine fields into the 4th column until what would be the start of the 5th column\n",
    "                fields[3:10] = [' '.join(fields[3:10])]\n",
    "                # Ensure no more than 11 fields before processing the 10th column\n",
    "                fields = fields[:11]\n",
    "\n",
    "            # Now, handle extra fields for the 10th column (statesummary)\n",
    "            if len(fields) > 11:\n",
    "                # Combine fields beyond the 11th into the 10th field\n",
    "                fields[10:] = [' '.join(fields[10:])]\n",
    "                fields = fields[:11]\n",
    "            \n",
    "            processed_rows.append(fields)\n",
    "            \n",
    "    # Create a DataFrame for each file's processed rows with the defined column names\n",
    "    df = pd.DataFrame(processed_rows, columns=column_names)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames from each file into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BillID</th>\n",
       "      <th>StateCode</th>\n",
       "      <th>StateBillID</th>\n",
       "      <th>ShortBillName</th>\n",
       "      <th>Created</th>\n",
       "      <th>SponsorParty</th>\n",
       "      <th>billtype</th>\n",
       "      <th>status</th>\n",
       "      <th>CommitteeCategories</th>\n",
       "      <th>statesummary</th>\n",
       "      <th>BillText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623584</td>\n",
       "      <td>MI</td>\n",
       "      <td>SR0039</td>\n",
       "      <td>\"A resolution to recognize April 27  2023  as ...</td>\n",
       "      <td>Signed/Enacted/Adopted</td>\n",
       "      <td></td>\n",
       "      <td>\"A RESOLUTION TO RECOGNIZE APRIL 27</td>\n",
       "      <td>2023</td>\n",
       "      <td>AS SURVIVORSSPEAK MICHIGAN DAY</td>\n",
       "      <td>A PART OF NATIONAL CRIME VICTIMS RIGHTS WEEK\"</td>\n",
       "      <td>\"Michigan MI SR 0039 MI SR0039 MISR0039 MI SR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1623654</td>\n",
       "      <td>AK</td>\n",
       "      <td>HB181</td>\n",
       "      <td>State Commission For Civil Rights 2023-04-26 1...</td>\n",
       "      <td>\"Alaska AK HB 181 AK HB181 AKHB181 AK HB 181 A...</td>\n",
       "      <td>cause includes incompetence</td>\n",
       "      <td>neglect of duty</td>\n",
       "      <td>and misconduct  in  office</td>\n",
       "      <td>and  public  statements  and  public  or  pr...</td>\n",
       "      <td>the governor shall provide a copy of the char...</td>\n",
       "      <td>or by counsel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1623695</td>\n",
       "      <td>MI</td>\n",
       "      <td>HB4474</td>\n",
       "      <td>Crime victims: other; elements for commission ...</td>\n",
       "      <td>entitled\\\"The Michigan penal code</td>\n",
       "      <td>\\\"by amending section 147b (MCL 750.147b)</td>\n",
       "      <td>as added by 1988 PA 371.\"</td>\n",
       "      <td>\"Michigan MI HB 4474 MI HB4474 MIHB4474 MI HB ...</td>\n",
       "      <td>entitled\\\"The Michigan penal code</td>\n",
       "      <td>\\\"by amending section 147b (MCL 750.147b)</td>\n",
       "      <td>as added by 1988 PA 371.   \\t      \\t\\t   \\t\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1623696</td>\n",
       "      <td>ME</td>\n",
       "      <td>LD1833</td>\n",
       "      <td>\"An Act to Amend the Definition of \\\"Education...</td>\n",
       "      <td>\"Maine ME LD 1833 ME LD1833 MELD1833 ME LD 183...</td>\n",
       "      <td>sub-§2-A</td>\n",
       "      <td>as amended by PL 1995</td>\n",
       "      <td>c. 393</td>\n",
       "      <td>§4</td>\n",
       "      <td>is further amended to read:    2-A.  Educatio...</td>\n",
       "      <td>any public  post-secondarypostsecondary  inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1623715</td>\n",
       "      <td>IL</td>\n",
       "      <td>SR0225</td>\n",
       "      <td>SIKH HERITAGE MONTH 2023-04-26 14:31:05.063000...</td>\n",
       "      <td>\"Illinois IL SR 0225 IL SR0225 ILSR0225 IL SR ...</td>\n",
       "      <td>The United States is enriched by the diversit...</td>\n",
       "      <td>The Sikh community</td>\n",
       "      <td>which originated in Punjab</td>\n",
       "      <td>India and began immigrating into the United S...</td>\n",
       "      <td>has played an important role in developing Il...</td>\n",
       "      <td>Sikhism is the world's fifth-largest religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>1521088</td>\n",
       "      <td>NY</td>\n",
       "      <td>S01584</td>\n",
       "      <td>\"Prohibits certain student organizations which...</td>\n",
       "      <td>\"AN ACT to amend the education law</td>\n",
       "      <td>in relation to certain student organizations ...</td>\n",
       "      <td>\"New York NY S 01584 NY S01584 NYS01584 NY S 1...</td>\n",
       "      <td>CUNY or community colleges.  AN ACT to amend ...</td>\n",
       "      <td>in relation to certain student organizations ...</td>\n",
       "      <td>represented in Senate and Assembly</td>\n",
       "      <td>do enact as follows:   \\t\\t   Section 1. Sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>1521119</td>\n",
       "      <td>NY</td>\n",
       "      <td>S01532</td>\n",
       "      <td>Requires the board of education and the truste...</td>\n",
       "      <td>in relation to requiring the board of educati...</td>\n",
       "      <td>\"New York NY S 01532 NY S01532 NYS01532 NY S 1...</td>\n",
       "      <td>in relation to requiring the board of educati...</td>\n",
       "      <td>represented in Senate and Assembly</td>\n",
       "      <td>do enact as follows:   \\t\\t   Section 1. Sect...</td>\n",
       "      <td>but are not limited to policies which: a. ens...</td>\n",
       "      <td>including  using  pronouns and  names  consi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>1521185</td>\n",
       "      <td>NY</td>\n",
       "      <td>S01565</td>\n",
       "      <td>Requires pet dealers and pet shops to provide ...</td>\n",
       "      <td>in relation to requiring pet dealers and pet ...</td>\n",
       "      <td>\"New York NY S 01565 NY S01565 NYS01565 NY S 1...</td>\n",
       "      <td>in relation to requiring pet dealers and pet ...</td>\n",
       "      <td>represented in Senate and Assembly</td>\n",
       "      <td>do enact as follows:   \\t\\t   Section 1. This...</td>\n",
       "      <td>as added by chapter  259 of the laws of 2000</td>\n",
       "      <td>subdivision 1 as amended by chapter 110 of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1521196</td>\n",
       "      <td>NY</td>\n",
       "      <td>S01556</td>\n",
       "      <td>\"Relates to an increase in punishment for cert...</td>\n",
       "      <td>\"AN ACT to amend the penal law</td>\n",
       "      <td>in relation to on duty auxiliary police offic...</td>\n",
       "      <td>\"New York NY S 01556 NY S01556 NYS01556 NY S 1...</td>\n",
       "      <td>assault or menacing of such officer.  AN ACT ...</td>\n",
       "      <td>in relation to on duty auxiliary police offic...</td>\n",
       "      <td>represented in Senate and Assembly</td>\n",
       "      <td>do enact as follows:   \\t\\t   Section 1. Sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>1521278</td>\n",
       "      <td>TN</td>\n",
       "      <td>SB0168</td>\n",
       "      <td>\"AN ACT to amend Tennessee Code Annotated  Tit...</td>\n",
       "      <td>Justice</td>\n",
       "      <td>\"As introduced</td>\n",
       "      <td>enacts the \\\"Free All Cannabis for Tennessean...</td>\n",
       "      <td>processing</td>\n",
       "      <td>and retail sale of marijuana and marijuana pr...</td>\n",
       "      <td>\"Tennessee TN SB 0168 TN SB0168 TNSB0168 TN SB...</td>\n",
       "      <td>Title 4; Title 29; Title 33; Title 38; Title ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BillID StateCode StateBillID  \\\n",
       "0    1623584        MI      SR0039   \n",
       "1    1623654        AK       HB181   \n",
       "2    1623695        MI      HB4474   \n",
       "3    1623696        ME      LD1833   \n",
       "4    1623715        IL      SR0225   \n",
       "..       ...       ...         ...   \n",
       "325  1521088        NY      S01584   \n",
       "326  1521119        NY      S01532   \n",
       "327  1521185        NY      S01565   \n",
       "328  1521196        NY      S01556   \n",
       "329  1521278        TN      SB0168   \n",
       "\n",
       "                                         ShortBillName  \\\n",
       "0    \"A resolution to recognize April 27  2023  as ...   \n",
       "1    State Commission For Civil Rights 2023-04-26 1...   \n",
       "2    Crime victims: other; elements for commission ...   \n",
       "3    \"An Act to Amend the Definition of \\\"Education...   \n",
       "4    SIKH HERITAGE MONTH 2023-04-26 14:31:05.063000...   \n",
       "..                                                 ...   \n",
       "325  \"Prohibits certain student organizations which...   \n",
       "326  Requires the board of education and the truste...   \n",
       "327  Requires pet dealers and pet shops to provide ...   \n",
       "328  \"Relates to an increase in punishment for cert...   \n",
       "329  \"AN ACT to amend Tennessee Code Annotated  Tit...   \n",
       "\n",
       "                                               Created  \\\n",
       "0                               Signed/Enacted/Adopted   \n",
       "1    \"Alaska AK HB 181 AK HB181 AKHB181 AK HB 181 A...   \n",
       "2                    entitled\\\"The Michigan penal code   \n",
       "3    \"Maine ME LD 1833 ME LD1833 MELD1833 ME LD 183...   \n",
       "4    \"Illinois IL SR 0225 IL SR0225 ILSR0225 IL SR ...   \n",
       "..                                                 ...   \n",
       "325                 \"AN ACT to amend the education law   \n",
       "326   in relation to requiring the board of educati...   \n",
       "327   in relation to requiring pet dealers and pet ...   \n",
       "328                     \"AN ACT to amend the penal law   \n",
       "329                                            Justice   \n",
       "\n",
       "                                          SponsorParty  \\\n",
       "0                                                        \n",
       "1                          cause includes incompetence   \n",
       "2            \\\"by amending section 147b (MCL 750.147b)   \n",
       "3                                             sub-§2-A   \n",
       "4     The United States is enriched by the diversit...   \n",
       "..                                                 ...   \n",
       "325   in relation to certain student organizations ...   \n",
       "326  \"New York NY S 01532 NY S01532 NYS01532 NY S 1...   \n",
       "327  \"New York NY S 01565 NY S01565 NYS01565 NY S 1...   \n",
       "328   in relation to on duty auxiliary police offic...   \n",
       "329                                     \"As introduced   \n",
       "\n",
       "                                              billtype  \\\n",
       "0                  \"A RESOLUTION TO RECOGNIZE APRIL 27   \n",
       "1                                      neglect of duty   \n",
       "2                            as added by 1988 PA 371.\"   \n",
       "3                                as amended by PL 1995   \n",
       "4                                   The Sikh community   \n",
       "..                                                 ...   \n",
       "325  \"New York NY S 01584 NY S01584 NYS01584 NY S 1...   \n",
       "326   in relation to requiring the board of educati...   \n",
       "327   in relation to requiring pet dealers and pet ...   \n",
       "328  \"New York NY S 01556 NY S01556 NYS01556 NY S 1...   \n",
       "329   enacts the \\\"Free All Cannabis for Tennessean...   \n",
       "\n",
       "                                                status  \\\n",
       "0                                                 2023   \n",
       "1                           and misconduct  in  office   \n",
       "2    \"Michigan MI HB 4474 MI HB4474 MIHB4474 MI HB ...   \n",
       "3                                               c. 393   \n",
       "4                           which originated in Punjab   \n",
       "..                                                 ...   \n",
       "325   CUNY or community colleges.  AN ACT to amend ...   \n",
       "326                 represented in Senate and Assembly   \n",
       "327                 represented in Senate and Assembly   \n",
       "328   assault or menacing of such officer.  AN ACT ...   \n",
       "329                                         processing   \n",
       "\n",
       "                                   CommitteeCategories  \\\n",
       "0                       AS SURVIVORSSPEAK MICHIGAN DAY   \n",
       "1      and  public  statements  and  public  or  pr...   \n",
       "2                    entitled\\\"The Michigan penal code   \n",
       "3                                                   §4   \n",
       "4     India and began immigrating into the United S...   \n",
       "..                                                 ...   \n",
       "325   in relation to certain student organizations ...   \n",
       "326   do enact as follows:   \\t\\t   Section 1. Sect...   \n",
       "327   do enact as follows:   \\t\\t   Section 1. This...   \n",
       "328   in relation to on duty auxiliary police offic...   \n",
       "329   and retail sale of marijuana and marijuana pr...   \n",
       "\n",
       "                                          statesummary  \\\n",
       "0       A PART OF NATIONAL CRIME VICTIMS RIGHTS WEEK\"   \n",
       "1     the governor shall provide a copy of the char...   \n",
       "2            \\\"by amending section 147b (MCL 750.147b)   \n",
       "3     is further amended to read:    2-A.  Educatio...   \n",
       "4     has played an important role in developing Il...   \n",
       "..                                                 ...   \n",
       "325                 represented in Senate and Assembly   \n",
       "326   but are not limited to policies which: a. ens...   \n",
       "327       as added by chapter  259 of the laws of 2000   \n",
       "328                 represented in Senate and Assembly   \n",
       "329  \"Tennessee TN SB 0168 TN SB0168 TNSB0168 TN SB...   \n",
       "\n",
       "                                              BillText  \n",
       "0    \"Michigan MI SR 0039 MI SR0039 MISR0039 MI SR ...  \n",
       "1                                        or by counsel  \n",
       "2     as added by 1988 PA 371.   \\t      \\t\\t   \\t\\...  \n",
       "3     any public  post-secondarypostsecondary  inst...  \n",
       "4        Sikhism is the world's fifth-largest religion  \n",
       "..                                                 ...  \n",
       "325   do enact as follows:   \\t\\t   Section 1. Sect...  \n",
       "326    including  using  pronouns and  names  consi...  \n",
       "327   subdivision 1 as amended by chapter 110 of th...  \n",
       "328   do enact as follows:   \\t\\t   Section 1. Sect...  \n",
       "329   Title 4; Title 29; Title 33; Title 38; Title ...  \n",
       "\n",
       "[330 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path and get all CSV files\n",
    "csv_file_path = \"../data_storage/legislation/complete_full_text.csv/*.csv\"\n",
    "csv_files = glob.glob(csv_file_path)\n",
    "\n",
    "# Define your column names here based on your dataset's requirements\n",
    "column_names = ['BillID', 'StateCode', 'StateBillID', 'ShortBillName', 'Created', 'SponsorParty', 'billtype', 'status', 'CommitteeCategories', 'statesummary', 'BillText']\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    processed_rows = []\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # Skip the first line if it's headers\n",
    "        for line in f:\n",
    "            # Initialize an empty list to collect the split fields\n",
    "            fields = []\n",
    "            field_start = 0\n",
    "            in_quotes = False\n",
    "            for i, char in enumerate(line):\n",
    "                # Toggle in_quotes status on quote\n",
    "                if char == '\"':\n",
    "                    in_quotes = not in_quotes\n",
    "                # Split on commas not within quotes\n",
    "                elif char == ',' and not in_quotes:\n",
    "                    fields.append(line[field_start:i])\n",
    "                    field_start = i + 1\n",
    "            # Add the last field\n",
    "            fields.append(line[field_start:].strip())\n",
    "\n",
    "            # Handle excess fields by combining them into the appropriate columns\n",
    "            if len(fields) > 11:\n",
    "                fields[3] = ','.join(fields[3:10])  # Combine into the 4th field\n",
    "                fields[10] = ','.join(fields[10:])  # Combine remaining fields into the 11th\n",
    "                fields = fields[:11]  # Keep only the first 11 fields\n",
    "            \n",
    "            processed_rows.append(fields)\n",
    "\n",
    "    # Create a DataFrame for the processed rows for this file\n",
    "    df = pd.DataFrame(processed_rows, columns=column_names)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv(\"../data_storage/legislation/complete_cleaned_full_text.csv\")\n",
    "combined_df.to_csv(\"../data_storage/legislation/complete_cleaned_full_text.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-publication-patterns-public",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
