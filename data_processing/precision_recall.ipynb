{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data (adjust the path as needed)\n",
    "wapo_data = pd.read_csv('../benchmark_data/fatal-police-shootings-data.csv')\n",
    "police_dept = pd.read_csv('../benchmark_data/fatal-police-shootings-agencies.csv')\n",
    "# print(wapo_data)\n",
    "# Define common suffixes to ignore\n",
    "common_suffixes = ['Jr', 'Sr', 'II', 'III', 'IV', 'V']\n",
    "\n",
    "# Define a function that processes each name\n",
    "def process_name(name):\n",
    "    # Ensure name is a string and not NaN or any float value\n",
    "    if pd.isnull(name):\n",
    "        # Handle NaN values or any other non-string values\n",
    "        return [None, None]  # Or return ['Unknown', 'Unknown'] based on your preference\n",
    "    else:\n",
    "        # Convert to string in case it's not (handles numeric values)\n",
    "        name = str(name)\n",
    "        # Split the name into parts\n",
    "        parts = name.split()\n",
    "        # Check and construct the name ignoring common suffixes and middle names\n",
    "        if parts[-1] in common_suffixes and len(parts) > 2:\n",
    "            first_last_name = f\"{parts[0]} {parts[-2]}\"\n",
    "        elif len(parts) > 2:\n",
    "            first_last_name = f\"{parts[0]} {parts[-1]}\"\n",
    "        else:\n",
    "            first_last_name = name\n",
    "        # Return a list with the original name, the first-last name version, and the version ignoring suffixes\n",
    "        return [name, first_last_name]\n",
    "\n",
    "# Apply the function to the 'name' column and create a new column with the results\n",
    "wapo_data['names'] = wapo_data['name'].apply(process_name)\n",
    "# wapo_data['date'] = pd.to_datetime(wapo_data['date'])\n",
    "wapo_data = wapo_data[(wapo_data['date'] >= '2023-12-01') & (wapo_data['date'] <= '2023-12-15')]\n",
    "\n",
    "result = pd.read_csv('../data_storage/benchmark/benchmark_large.csv')\n",
    "# result['publication_date'] = pd.to_datetime(result['publication_date'])\n",
    "wapo_data.reset_index(drop=True, inplace=True)\n",
    "result_hit = result[result['Hit?']==\"Y\"]\n",
    "print(result_hit.shape)\n",
    "print(result.shape)\n",
    "# wapo_data\n",
    "\n",
    "import numpy as np\n",
    "from dateutil import tz\n",
    "\n",
    "\n",
    "def parse_closest_date(wapo_date, publication_dates_str):\n",
    "    # Ensure wapo_date is tz-naive\n",
    "    wapo_date = wapo_date.replace(tzinfo=None)\n",
    "    \n",
    "    # Split the string into individual dates and remove any whitespace\n",
    "    date_str_list = publication_dates_str.strip().replace('(', '').replace(')', '').split(',')\n",
    "    \n",
    "    # Convert string dates to datetime and ensure they are tz-naive\n",
    "    date_diffs = [abs(wapo_date - pd.to_datetime(date_str.strip(), utc=True).replace(tzinfo=None)).days for date_str in date_str_list]\n",
    "    \n",
    "    # Return the date with the minimum difference\n",
    "    min_diff_index = np.argmin(date_diffs)\n",
    "    return pd.to_datetime(date_str_list[min_diff_index].strip(), utc=True).replace(tzinfo=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([ 2,  5, 11, 13, 14, 20, 21, 22, 24, 26, 29, 33, 34, 37, 38, 40, 42, 45,\n",
      "       48, 50, 52, 54, 55],\n",
      "      dtype='int64')\n",
      "True conditions: 23 out of 57 rows.\n"
     ]
    }
   ],
   "source": [
    "# Assuming the preparation steps have been done as in your provided code\n",
    "\n",
    "# Helper function to check conditions\n",
    "def check_conditions(row, results_df, police_dept):\n",
    "    # Ensure 'names' is a list of strings, filtering out any None values\n",
    "    names = [str(name) for name in row['names'] if pd.notnull(name)]\n",
    "    \n",
    "    # Check if any name in 'names' appears in 'snippet'\n",
    "    for name in names:\n",
    "        if any(name in str(snippet) for snippet in results_df['snippet']):\n",
    "            return True\n",
    "    \n",
    "    # Convert 'date' to datetime for comparison\n",
    "    wapo_date = pd.to_datetime(row['date'])\n",
    "    \n",
    "    # Ensure 'city' and 'county' are strings, handling potential NaN values\n",
    "    city = str(row['city']) if pd.notnull(row['city']) else \"\"\n",
    "    county = str(row['county']) if pd.notnull(row['county']) else \"\"\n",
    "    \n",
    "    # Check if 'city' appears in 'snippet' and dates within 7 days\n",
    "    city_condition = results_df.apply(lambda x: city in str(x['snippet']) and abs((wapo_date - parse_closest_date(wapo_date, x['publication_date'])).days) <= 7, axis=1)\n",
    "    if city_condition.any():\n",
    "        return True\n",
    "    \n",
    "    # Check if 'county' appears in 'snippet' and dates within 5 days\n",
    "    county_condition = results_df.apply(lambda x: county in str(x['snippet']) and abs((wapo_date - parse_closest_date(wapo_date, x['publication_date'])).days) <= 5, axis=1)\n",
    "    if county_condition.any():\n",
    "        return True\n",
    "    \n",
    "    agency_id = row['agency_ids']\n",
    "    if pd.notnull(agency_id):\n",
    "        # Find the corresponding name in 'police_dept' for the given 'agency_id'\n",
    "        agency_name = police_dept.loc[police_dept['id'] == agency_id, 'name'].values\n",
    "        if len(agency_name) > 0:  # Ensure there is a match\n",
    "            agency_name = str(agency_name[0])  # Convert to string in case it's not\n",
    "            # Check if the agency name appears in 'snippet' and dates within 5 days\n",
    "            agency_condition = results_df.apply(lambda x: agency_name in str(x['snippet']) and abs((wapo_date - parse_closest_date(wapo_date, x['publication_date'])).days) <= 5, axis=1)\n",
    "            if agency_condition.any():\n",
    "                return True\n",
    "\n",
    "    # If none of the conditions are met\n",
    "    return False\n",
    "\n",
    "\n",
    "# Apply the helper function to each row in wapo_data\n",
    "wapo_data['condition_met'] = wapo_data.apply(check_conditions, results_df=result, police_dept=police_dept, axis=1)\n",
    "\n",
    "idx = wapo_data.index[wapo_data['condition_met']]\n",
    "print(idx)\n",
    "\n",
    "# Print summary\n",
    "true_count = wapo_data['condition_met'].sum()\n",
    "total_rows = len(wapo_data)\n",
    "print(f\"True conditions: {true_count} out of {total_rows} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.2.2. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Pooling.__init__() got an unexpected keyword argument 'pooling_mode_weightedmean_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 60\u001b[0m\n\u001b[1;32m     46\u001b[0m instruction_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepresent the news articles for retrieval:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# # Using the text-embedding-3-large model to generate embeddings\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# response = openai_client.embeddings.create(\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#     input=query_text,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# query_vector = query_model.encode(query_text).tolist()\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m query_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnomic-ai/nomic-embed-text-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m query_vector \u001b[38;5;241m=\u001b[39m query_model\u001b[38;5;241m.\u001b[39mencode(query_text, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# query_vector = embed_model.encode([[instruction_prompt,query_text]]).tolist()\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# query_vector = [item for sublist in query_vector for item in sublist]\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/semantic-search/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[1;32m     88\u001b[0m                             cache_dir\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[1;32m     89\u001b[0m                             library_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     90\u001b[0m                             library_version\u001b[38;5;241m=\u001b[39m__version__,\n\u001b[1;32m     91\u001b[0m                             ignore_files\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflax_model.msgpack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrust_model.ot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     92\u001b[0m                             use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):    \u001b[38;5;66;03m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m#Load with AutoModel\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[0;32m~/.virtualenvs/semantic-search/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module_config \u001b[38;5;129;01min\u001b[39;00m modules_config:\n\u001b[1;32m    839\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m import_from_string(module_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 840\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m     modules[module_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m modules\n",
      "File \u001b[0;32m~/.virtualenvs/semantic-search/lib/python3.11/site-packages/sentence_transformers/models/Pooling.py:120\u001b[0m, in \u001b[0;36mPooling.load\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m fIn:\n\u001b[1;32m    118\u001b[0m     config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fIn)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPooling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Pooling.__init__() got an unexpected keyword argument 'pooling_mode_weightedmean_tokens'"
     ]
    }
   ],
   "source": [
    "\n",
    "from waybacknews.searchapi import SearchApiClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "from retrying import retry\n",
    "import requests_cache\n",
    "from tqdm import tqdm\n",
    "# import mediacloud.api\n",
    "# from newspaper import Article\n",
    "# import unicodedata\n",
    "import concurrent.futures\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from openai import OpenAI\n",
    "\n",
    "import weaviate\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "weaviate_api_key = os.getenv('WEAVIATE_API_KEY')\n",
    "weaviate_url = os.getenv('WEAVIATE_URL')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url = weaviate_url,\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=weaviate_api_key), \n",
    "    additional_headers = {\n",
    "        \"X-OpenAI-Api-Key\": OPENAI_API_KEY\n",
    "    }\n",
    ")\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "query_text = \"\"\"\n",
    "Breaking news coverage on recent police shooting incidents in the United States leading to fatalities of the victims. The incidents are about police officer, deputy, sheriff, trooper, cop who fired shots and killed someone.\n",
    "Do not include aggregated summary, list, or archive of incidents happened in the past. Do not include if it's about a past, not recent incident. Only include if the story mentioned the death of the victim.\n",
    "Do not include if it's coverage on legal proceedings, court cases, or trials of a past incident. Do not include if it's about the aftermath of the incident, such as protests, rallies, or demonstrations on a past incident.\n",
    "Do not include if it happened in a foreign country, only include if it's about the United States.\n",
    "\"\"\"\n",
    "instruction_prompt = \"Represent the news articles for retrieval:\"\n",
    "\n",
    "# # Using the text-embedding-3-large model to generate embeddings\n",
    "# response = openai_client.embeddings.create(\n",
    "#     input=query_text,\n",
    "#     model=\"text-embedding-3-small\"\n",
    "# )\n",
    "# query_vector = response.data[0].embedding\n",
    "\n",
    "# query_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\")\n",
    "\n",
    "# query_vector = query_model.encode(query_text).tolist()\n",
    "\n",
    "\n",
    "query_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\")\n",
    "query_vector = query_model.encode(query_text, convert_to_tensor=True)\n",
    "\n",
    "# query_vector = embed_model.encode([[instruction_prompt,query_text]]).tolist()\n",
    "# query_vector = [item for sublist in query_vector for item in sublist]\n",
    "print(\"vector:\", query_vector.__len__())\n",
    "\n",
    "get_articles_group = f\"\"\"\n",
    "{{\n",
    "  Get {{\n",
    "    Article(\n",
    "      nearVector: {{\n",
    "        vector: {query_vector}\n",
    "      }},\n",
    "      group: {{\n",
    "        type: merge,\n",
    "        force: 0\n",
    "      }},\n",
    "      limit: 80\n",
    "    ) {{\n",
    "      title,\n",
    "      publication_date,\n",
    "      snippet\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "query_result = client.query.raw(get_articles_group)\n",
    "# save to csv\n",
    "df = pd.DataFrame(query_result['data']['Get']['Article'])\n",
    "df.to_csv(f'./data_storage/{timestamp}_test_weaviate_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
