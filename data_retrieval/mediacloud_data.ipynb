{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from waybacknews.searchapi import SearchApiClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "from retrying import retry\n",
    "import requests_cache\n",
    "from tqdm import tqdm\n",
    "# import mediacloud.api\n",
    "from newspaper import Article\n",
    "# import unicodedata\n",
    "import concurrent.futures\n",
    "from utils.utils import split_into_chunks, fetch_snippet, sanitize_snippet, get_snippet_from_wayback_machine\n",
    "\n",
    "import weaviate\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up weaviate client\n",
    "OPENAI_APIKEY = os.environ['OPENAI_API_KEY']\n",
    "# HuggingFace_APIKEY = os.environ['HuggingFace_API_KEY']\n",
    "client = weaviate.Client(\n",
    "    url = \"https://semantic-search-cluster-mp9jmm6o.weaviate.network\",  # Replace with your endpoint\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=\"kEzuSpJyK8J7IDLLsmzoBlH8mOtkrfkCjIH6\"),  # Replace w/ your Weaviate instance API key\n",
    "    additional_headers = {\n",
    "        \"X-OpenAI-Api-Key\": OPENAI_APIKEY\n",
    "        # \"X-Huggingface-Api-Key\": HuggingFace_APIKEY\n",
    "    }\n",
    ")\n",
    "\n",
    "if client.schema.exists(\"Article\"):\n",
    "    client.schema.delete_class(\"Article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for raw text\n",
    "# class_obj = {\n",
    "#     \"class\": \"Article\",\n",
    "#     \"vectorizer\": \"text2vec-huggingface\",  # \"text2vec-openai\"\n",
    "#     \"moduleConfig\": {\n",
    "#         \"text2vec-huggingface\": {\n",
    "#             \"model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#         #     \"options\": {\n",
    "#         #         \"waitForModel\": True,\n",
    "#         #         \"useGPU\": True,\n",
    "#         #         \"useCache\": True}\n",
    "#         },\n",
    "#         # \"text2vec-openai\": {},\n",
    "#         \"generative-openai\": {}  # Ensure the `generative-openai` module is used for generative queries\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# for pre-vectorized data\n",
    "class_obj = {\n",
    "    \"class\": \"Article\",\n",
    "    \"vectorizer\": \"none\",\n",
    "    \"moduleConfig\": {\n",
    "        \"generative-openai\": {}  # Ensure the `generative-openai` module is used for generative queries\n",
    "    }\n",
    "}\n",
    "\n",
    "client.schema.create_class(class_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection has 8913 sources\n",
      "Number of sources: 8913\n",
      "Found 696 articles\n",
      "Found 402 articles\n",
      "Found 164 articles\n",
      "Found 45 articles\n",
      "Found 56 articles\n",
      "Found 26 articles\n",
      "Found 25 articles\n",
      "Found 11 articles\n",
      "Found 7 articles\n",
      "(1432, 9)\n",
      "after dropping duplicates:  (679, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 679/679 [02:49<00:00,  4.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(679, 10)\n",
      "after dropping null snippets:  (679, 10)\n",
      "Data retrieval complete. Results saved to '../data_storage/2023-07-01 00:00:00.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import mediacloud.api\n",
    "collection_id = \"38379429\"\n",
    "api = SearchApiClient(\"mediacloud\")\n",
    "\n",
    "SOURCES_PER_PAGE = 100  # the number of sources retrieved per page\n",
    "mc_directory = mediacloud.api.DirectoryApi('e85cce24da8b73eaa05329d258146c044ef055db')\n",
    "sources = []\n",
    "offset = 0   # offset for paging through\n",
    "while True:\n",
    "    # grab a page of sources in the collection\n",
    "    response = mc_directory.source_list(collection_id=collection_id, limit=SOURCES_PER_PAGE, offset=offset)\n",
    "    # add it to our running list of all the sources in the collection\n",
    "    sources += response['results']\n",
    "    # if there is no next page then we're done so bail out\n",
    "    if response['next'] is None:\n",
    "        break\n",
    "    # otherwise setup to fetch the next page of sources\n",
    "    offset += len(response['results'])\n",
    "print(\"Collection has {} sources\".format(len(sources)))\n",
    "all_domains = [s['name'] for s in sources]\n",
    "\n",
    "# Cleaning up domains\n",
    "cleaned_domains = [\n",
    "    domain.replace('https://www.', '')\n",
    "          .replace('http://www.', '')\n",
    "          .replace('https://', '')\n",
    "          .replace('http://', '')\n",
    "          .replace('#spider', '')\n",
    "          .replace('/#spider', '')\n",
    "          .rstrip('/')\n",
    "    for domain in all_domains\n",
    "    if domain  # Ensure the domain is not None or empty\n",
    "]\n",
    "print(f\"Number of sources: {len(cleaned_domains)}\")\n",
    "domains_df = pd.DataFrame(cleaned_domains, columns=['Domain'])\n",
    "# Save the DataFrame to a CSV file\n",
    "# domains_df.to_csv('domains.csv', index=False)\n",
    "\n",
    "domain_chunks = list(split_into_chunks(cleaned_domains, 1000))\n",
    "\n",
    "# Enable requests cache\n",
    "requests_cache.install_cache('article_cache', backend='filesystem', expire_after=3600)\n",
    "\n",
    "\n",
    "# Query parameters\n",
    "# query_term = '(\"police shooting\" OR \"shot by police\" OR \"police shot\" OR \"officer-involved shooting\" OR \"police-involved shooting\" OR \"police officer shooting\" OR \"police shot\" OR \"officer shot\")'\n",
    "query_term = '(\"police shooting\" OR \"shot by police\" OR \"police shot\" OR \"officer-involved shooting\" OR \"police-involved shooting\" OR \"police officer shooting\" OR \"officer shot\" OR \"deputy shot\" OR \"sheriff shot\" OR \"cop shot\" OR \"trooper shot\" OR \"shot by officer\" OR \"shot by deputy\" OR \"shot by sheriff\" OR \"shot by cop\" OR \"shot by trooper\" OR \\\n",
    "    \"killed by police\" OR \"killed by officer\" OR \"killed by deputy\" OR \"killed by sheriff\" OR \"killed by cop\" OR \"killed by trooper\")'\n",
    "\n",
    "# query_term = 'police AND shot'\n",
    "start = datetime(2023, 7, 1) #11/6 - 11/15\n",
    "end = datetime(2023, 7, 15) \n",
    "language = \"en\"\n",
    "\n",
    "# DataFrame to store combined results\n",
    "combined_results = pd.DataFrame()\n",
    "results_list = []\n",
    "\n",
    "for chunk in domain_chunks:\n",
    "    domains_str = f\"domain:({' OR '.join(chunk)})\"\n",
    "    query = f\"{query_term} AND language:{language} AND {domains_str}\"\n",
    "    \n",
    "    # Perform the search with the current chunk\n",
    "    articles = []\n",
    "    for list_of_articles in api.all_articles(query, start, end):\n",
    "        articles.extend(list_of_articles)\n",
    "        print(f\"Found {len(articles)} articles\")\n",
    "    \n",
    "    if articles:\n",
    "        chunk_results = pd.DataFrame(articles)\n",
    "        results_list.append(chunk_results)\n",
    "    \n",
    "# Concatenate all DataFrames in the list\n",
    "combined_results = pd.concat(results_list, ignore_index=True)\n",
    "# results = pd.DataFrame(articles).sort_values(by='publication_date', ascending=False)\n",
    "combined_results.sort_values(by='publication_date', ascending=False, inplace=True)\n",
    "print(combined_results.shape)\n",
    "combined_results.drop_duplicates(subset=['title'], keep='first', inplace=True)\n",
    "print(\"after dropping duplicates: \", combined_results.shape)\n",
    "combined_results.to_csv(f'../data_storage/{start}_no_snippet.csv', index=False)\n",
    "\n",
    "\n",
    "for index, article in tqdm(combined_results.iterrows(), total=combined_results.shape[0]):\n",
    "    # only use wayback machine\n",
    "    wayback_url = article['article_url']\n",
    "    snippet = get_snippet_from_wayback_machine(wayback_url)\n",
    "    if snippet:\n",
    "        sanitized_snippet = sanitize_snippet(snippet)\n",
    "        combined_results.loc[index, 'snippet'] = sanitized_snippet\n",
    "        \n",
    "    # or concurrently run mediacloud wayback machine and newspaper3k\n",
    "    # article_url = article['url']\n",
    "    # wayback_url = article['article_url']\n",
    "    # snippet, method_used = fetch_snippet(article_url, wayback_url)\n",
    "    # if snippet:\n",
    "    #     sanitized_snippet = sanitize_snippet(snippet)\n",
    "    #     combined_results.loc[index, 'snippet'] = sanitized_snippet\n",
    "    #     # print(f\"Snippet fetched using {method_used} method for URL: {article_url}\")\n",
    "\n",
    "print(combined_results.shape)\n",
    "combined_results.dropna(subset=['snippet'], inplace=True)\n",
    "print(\"after dropping null snippets: \", combined_results.shape)\n",
    "combined_results.reset_index(drop=True, inplace=True)\n",
    "combined_results.to_csv(f'../data_storage/{start}.csv', index=False)\n",
    "print(f\"Data retrieval complete. Results saved to '../data_storage/{start}.csv'.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 256\n"
     ]
    }
   ],
   "source": [
    "# pre-vectorize the snippet to avoid using huggingface API which costs money\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Max Sequence Length:\", model.max_seq_length)\n",
    "model.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 679/679 [00:42<00:00, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(679, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "combined_results['snippet_vector'] = None\n",
    "\n",
    "for index, row in tqdm(combined_results.iterrows(), total=combined_results.shape[0]):\n",
    "    snippet = row['snippet']\n",
    "    snippet_vector = model.encode(snippet).tolist()\n",
    "\n",
    "    # Store embeddings in DataFrame\n",
    "    combined_results.at[index, 'snippet_vector'] = snippet_vector\n",
    "\n",
    "print(combined_results.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for raw text\n",
    "# print(combined_results.shape)\n",
    "# with client.batch(batch_size=100) as batch:\n",
    "#     for index, row in (combined_results.iterrows()):\n",
    "#         properties = {\n",
    "#             \"title\": row['title'],\n",
    "#             \"publication_date\": row['publication_date'],\n",
    "#             \"snippet\": row['snippet']\n",
    "#         }\n",
    "#         batch.add_data_object(properties, \"Article\")\n",
    "#         print(f\"Importing article: {index + 1}\")\n",
    "\n",
    "# for pre-vectorized data\n",
    "client.batch.configure(batch_size=100)\n",
    "with client.batch as batch:\n",
    "    for index, row in (combined_results.iterrows()):\n",
    "        properties = {\n",
    "            \"title\": row['title'],\n",
    "            \"publication_date\": row['publication_date'],\n",
    "            \"snippet\": row['snippet']\n",
    "        }\n",
    "        batch.add_data_object(properties, \"Article\", vector=row[\"snippet_vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphql (allows to group/merge similar results)\n",
    "query_text = \"Find articles about police shooting that resulted in death of the victims. The incident is about police officer,deputy, sheriff, trooper, cop who fatally fired shots and killed someone. The victims are dead.\"\n",
    "\n",
    "query_vector = model.encode(query_text).tolist()\n",
    "\n",
    "get_articles_group = f\"\"\"\n",
    "{{\n",
    "  Get {{\n",
    "    Article(\n",
    "      nearVector: {{\n",
    "        vector: {query_vector}\n",
    "      }},\n",
    "      group: {{\n",
    "        type: merge,\n",
    "        force: 0.15\n",
    "      }},\n",
    "      limit: 100\n",
    "    ) {{\n",
    "      title,\n",
    "      publication_date,\n",
    "      snippet,\n",
    "      _additional {{\n",
    "        generate(\n",
    "          singleResult: {{\n",
    "            prompt: \\\"\\\"\\\"Summarize the article: '{'{{snippet}}'}' in one sentence.\\\"\\\"\\\"\n",
    "          }}\n",
    "        ) {{\n",
    "          singleResult,\n",
    "          error\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "query_result = client.query.raw(get_articles_group)\n",
    "# save to csv\n",
    "df = pd.DataFrame(query_result['data']['Get']['Article'])\n",
    "df.to_csv(f'../data_storage/test_weaviate_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without grouping\n",
    "\n",
    "# nearText = {\"concepts\": [\"police shooting, shot by police, police shot, officer-involved shooting, police-involved shooting, police officer shooting, officer shot, deputy shot, sheriff shot, cop shot, trooper shot, shot by officer, shot by deputy, shot by sheriff, shot by cop, shot by trooper, killed by police, killed by officer, killed by deputy, killed by sheriff, killed by cop, killed by trooper\"]}\n",
    "# nearText = {\"concepts\": [\"a recent fatal police shooting. The incident is about police officer fired shots and killed someone. The victims are dead.\"]}\n",
    "# nearText = {\"concepts\": [\"Find articles about police shooting that resulted in death of the victims. The incident is about police officer fired shots and killed someone. The victims are dead.\"]}\n",
    "\n",
    "query_text = \"Find articles about police shooting that resulted in death of the victims. The incident is about police officer,deputy, sheriff, trooper, cop who fired shots and killed someone. The victims are dead. Example: Elk Grove Village police fatally shoot man they say was wielding knife Elk Grove police officers responded to reports of a man with a knife in the 200 block of Fern Drive, and once at the scene, a man exited a house with a knife, police said. He then confronted police in a nearby yard, and during the exchange, police shot him. Elk Grove police officers fatally shot a man whom they say was armed with a knife Friday night outside a home in the northwest suburb. Jack Murray, 24, was shot by officers about 4 p.m. in the 200 block of Fern Drive, according to police and the Cook County Medical Examiner√¢s Office. Officers were responding to reports of a man with a knife. Once officers arrived, Murray allegedly left a house with a knife and confronted police in a nearby yard, and during the exchange, police shot him. Murray was taken to a hospital where he was pronounced dead, police said. Three police officers were also taken to hospitals for observation. Afterward, police said there was no √¢active public safety risk.√¢ An attorney for Murray√¢s family said in a statement that they weren√¢t allowed back into their home late Friday, and that they had √¢no reason to believe that the anything inside the Murrays√¢ family home would be pertinent or relevant to any crime.√¢ √¢We hope to cooperate with the Elk Grove Village police to determine exactly what happened during this tragic incident,√¢ attorney Antonio Romanucci said. Elk Grove police are conducting a criminal investigation, and the regional Major Case Assistance Team is investigating the shooting.\"\n",
    "nearVector = {\"vector\": model.encode(query_text).tolist()}\n",
    "\n",
    "response = (\n",
    "    client.query\n",
    "    .get(\"Article\", [\"title\", \"publication_date\", \"snippet\"])\n",
    "    .with_near_vector(nearVector)  # Pass the nearVector_dict to with_near_vector\n",
    "    # .with_generate(single_prompt=\"Summarize {snippet} in one sentence.\")\n",
    "    .with_limit(50)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=4))\n",
    "# convert response to dataframe and save to csv\n",
    "df = pd.DataFrame(response['data']['Get']['Article'])\n",
    "df.to_csv(f'../data_storage/{start}_weaviate.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
