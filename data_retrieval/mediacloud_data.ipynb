{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from waybacknews.searchapi import SearchApiClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "from retrying import retry\n",
    "import requests_cache\n",
    "from tqdm import tqdm\n",
    "# import mediacloud.api\n",
    "from newspaper import Article\n",
    "# import unicodedata\n",
    "import concurrent.futures\n",
    "from utils.utils import split_into_chunks, fetch_snippet, sanitize_snippet, get_snippet_from_wayback_machine\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import weaviate\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up weaviate client\n",
    "OPENAI_APIKEY = os.environ['OPENAI_API_KEY']\n",
    "# HuggingFace_APIKEY = os.environ['HuggingFace_API_KEY']\n",
    "client = weaviate.Client(\n",
    "    url = \"https://semantic-search-cluster-mp9jmm6o.weaviate.network\",  # Replace with your endpoint\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=\"kEzuSpJyK8J7IDLLsmzoBlH8mOtkrfkCjIH6\"),  # Replace w/ your Weaviate instance API key\n",
    "    additional_headers = {\n",
    "        \"X-OpenAI-Api-Key\": OPENAI_APIKEY\n",
    "        # \"X-Huggingface-Api-Key\": HuggingFace_APIKEY\n",
    "    }\n",
    ")\n",
    "\n",
    "if client.schema.exists(\"Article\"):\n",
    "    client.schema.delete_class(\"Article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for raw text\n",
    "# class_obj = {\n",
    "#     \"class\": \"Article\",\n",
    "#     \"vectorizer\": \"text2vec-huggingface\",  # \"text2vec-openai\"\n",
    "#     \"moduleConfig\": {\n",
    "#         \"text2vec-huggingface\": {\n",
    "#             \"model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#         #     \"options\": {\n",
    "#         #         \"waitForModel\": True,\n",
    "#         #         \"useGPU\": True,\n",
    "#         #         \"useCache\": True}\n",
    "#         },\n",
    "#         # \"text2vec-openai\": {},\n",
    "#         \"generative-openai\": {}  # Ensure the `generative-openai` module is used for generative queries\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# for pre-vectorized data\n",
    "class_obj = {\n",
    "    \"class\": \"Article\",\n",
    "    \"vectorizer\": \"none\",\n",
    "    \"moduleConfig\": {\n",
    "        \"generative-openai\": {}  # Ensure the `generative-openai` module is used for generative queries\n",
    "    }\n",
    "}\n",
    "\n",
    "client.schema.create_class(class_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection has 8913 sources\n",
      "Number of sources: 8913\n",
      "Found 415 articles\n",
      "Found 353 articles\n",
      "Found 86 articles\n",
      "Found 23 articles\n",
      "Found 37 articles\n",
      "Found 29 articles\n",
      "Found 9 articles\n",
      "Found 6 articles\n",
      "Found 3 articles\n",
      "(961, 9)\n",
      "after dropping duplicates:  (495, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [02:24<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(495, 10)\n",
      "after dropping null snippets:  (495, 10)\n",
      "Data retrieval complete. Results saved to '../data_storage/2023-12-01 00:00:00.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import mediacloud.api\n",
    "collection_id = \"38379429\"\n",
    "api = SearchApiClient(\"mediacloud\")\n",
    "\n",
    "SOURCES_PER_PAGE = 100  # the number of sources retrieved per page\n",
    "mc_directory = mediacloud.api.DirectoryApi('e85cce24da8b73eaa05329d258146c044ef055db')\n",
    "sources = []\n",
    "offset = 0   # offset for paging through\n",
    "while True:\n",
    "    # grab a page of sources in the collection\n",
    "    response = mc_directory.source_list(collection_id=collection_id, limit=SOURCES_PER_PAGE, offset=offset)\n",
    "    # add it to our running list of all the sources in the collection\n",
    "    sources += response['results']\n",
    "    # if there is no next page then we're done so bail out\n",
    "    if response['next'] is None:\n",
    "        break\n",
    "    # otherwise setup to fetch the next page of sources\n",
    "    offset += len(response['results'])\n",
    "print(\"Collection has {} sources\".format(len(sources)))\n",
    "all_domains = [s['name'] for s in sources]\n",
    "\n",
    "# Cleaning up domains\n",
    "cleaned_domains = [\n",
    "    domain.replace('https://www.', '')\n",
    "          .replace('http://www.', '')\n",
    "          .replace('https://', '')\n",
    "          .replace('http://', '')\n",
    "          .replace('#spider', '')\n",
    "          .replace('/#spider', '')\n",
    "          .rstrip('/')\n",
    "    for domain in all_domains\n",
    "    if domain  # Ensure the domain is not None or empty\n",
    "]\n",
    "print(f\"Number of sources: {len(cleaned_domains)}\")\n",
    "domains_df = pd.DataFrame(cleaned_domains, columns=['Domain'])\n",
    "# Save the DataFrame to a CSV file\n",
    "# domains_df.to_csv('domains.csv', index=False)\n",
    "\n",
    "domain_chunks = list(split_into_chunks(cleaned_domains, 1000))\n",
    "\n",
    "# Enable requests cache\n",
    "requests_cache.install_cache('article_cache', backend='filesystem', expire_after=3600)\n",
    "\n",
    "\n",
    "# Query parameters\n",
    "# query_term = '(\"police shooting\" OR \"shot by police\" OR \"police shot\" OR \"officer-involved shooting\" OR \"police-involved shooting\" OR \"police officer shooting\" OR \"police shot\" OR \"officer shot\")'\n",
    "query_term = '(\"police shooting\" OR \"shot by police\" OR \"police shot\" OR \"officer-involved shooting\" OR \"police-involved shooting\" OR \"police officer shooting\" OR \"officer shot\" OR \"deputy shot\" OR \"sheriff shot\" OR \"cop shot\" OR \"trooper shot\" OR \"shot by officer\" OR \"shot by deputy\" OR \"shot by sheriff\" OR \"shot by cop\" OR \"shot by trooper\" OR \\\n",
    "    \"killed by police\" OR \"killed by officer\" OR \"killed by deputy\" OR \"killed by sheriff\" OR \"killed by cop\" OR \"killed by trooper\")'\n",
    "\n",
    "# query_term = 'police AND shot'\n",
    "start = datetime(2023, 12, 1) #11/6 - 11/15\n",
    "end = datetime(2023, 12, 15) \n",
    "language = \"en\"\n",
    "\n",
    "# DataFrame to store combined results\n",
    "combined_results = pd.DataFrame()\n",
    "results_list = []\n",
    "\n",
    "for chunk in domain_chunks:\n",
    "    domains_str = f\"domain:({' OR '.join(chunk)})\"\n",
    "    query = f\"{query_term} AND language:{language} AND {domains_str}\"\n",
    "    \n",
    "    # Perform the search with the current chunk\n",
    "    articles = []\n",
    "    for list_of_articles in api.all_articles(query, start, end):\n",
    "        articles.extend(list_of_articles)\n",
    "        print(f\"Found {len(articles)} articles\")\n",
    "    \n",
    "    if articles:\n",
    "        chunk_results = pd.DataFrame(articles)\n",
    "        results_list.append(chunk_results)\n",
    "    \n",
    "# Concatenate all DataFrames in the list\n",
    "combined_results = pd.concat(results_list, ignore_index=True)\n",
    "# results = pd.DataFrame(articles).sort_values(by='publication_date', ascending=False)\n",
    "combined_results.sort_values(by='publication_date', ascending=False, inplace=True)\n",
    "print(combined_results.shape)\n",
    "combined_results.drop_duplicates(subset=['title'], keep='first', inplace=True)\n",
    "print(\"after dropping duplicates: \", combined_results.shape)\n",
    "combined_results.to_csv(f'../data_storage/{start}_no_snippet.csv', index=False)\n",
    "\n",
    "\n",
    "for index, article in tqdm(combined_results.iterrows(), total=combined_results.shape[0]):\n",
    "    # only use wayback machine\n",
    "    wayback_url = article['article_url']\n",
    "    snippet = get_snippet_from_wayback_machine(wayback_url)\n",
    "    if snippet:\n",
    "        sanitized_snippet = sanitize_snippet(snippet)\n",
    "        combined_results.loc[index, 'snippet'] = sanitized_snippet\n",
    "        \n",
    "    # or concurrently run mediacloud wayback machine and newspaper3k\n",
    "    # article_url = article['url']\n",
    "    # wayback_url = article['article_url']\n",
    "    # snippet, method_used = fetch_snippet(article_url, wayback_url)\n",
    "    # if snippet:\n",
    "    #     sanitized_snippet = sanitize_snippet(snippet)\n",
    "    #     combined_results.loc[index, 'snippet'] = sanitized_snippet\n",
    "    #     # print(f\"Snippet fetched using {method_used} method for URL: {article_url}\")\n",
    "\n",
    "print(combined_results.shape)\n",
    "combined_results.dropna(subset=['snippet'], inplace=True)\n",
    "print(\"after dropping null snippets: \", combined_results.shape)\n",
    "combined_results.reset_index(drop=True, inplace=True)\n",
    "combined_results.to_csv(f'../data_storage/{start}.csv', index=False)\n",
    "print(f\"Data retrieval complete. Results saved to '../data_storage/{start}.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karenwang/.virtualenvs/semantic-search/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      ".gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 2.46MB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 1.19MB/s]\n",
      "README.md: 100%|██████████| 5.13k/5.13k [00:00<00:00, 22.7MB/s]\n",
      "config.json: 100%|██████████| 627/627 [00:00<00:00, 5.15MB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 1.22MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:12<00:00, 7.40MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 474kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 577kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 913kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 430/430 [00:00<00:00, 876kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 10.6MB/s]\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 3.09MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 384\n"
     ]
    }
   ],
   "source": [
    "# pre-vectorize the snippet to avoid using huggingface API which costs money\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('msmarco-MiniLM-L6-cos-v5')\n",
    "print(\"Max Sequence Length:\", model.max_seq_length)\n",
    "model.max_seq_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [00:29<00:00, 16.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(495, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "combined_results['snippet_vector'] = None\n",
    "\n",
    "for index, row in tqdm(combined_results.iterrows(), total=combined_results.shape[0]):\n",
    "    snippet = row['snippet']\n",
    "    snippet_vector = model.encode(snippet).tolist()\n",
    "\n",
    "    # Store embeddings in DataFrame\n",
    "    combined_results.at[index, 'snippet_vector'] = snippet_vector\n",
    "\n",
    "print(combined_results.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for raw text\n",
    "# print(combined_results.shape)\n",
    "# with client.batch(batch_size=100) as batch:\n",
    "#     for index, row in (combined_results.iterrows()):\n",
    "#         properties = {\n",
    "#             \"title\": row['title'],\n",
    "#             \"publication_date\": row['publication_date'],\n",
    "#             \"snippet\": row['snippet']\n",
    "#         }\n",
    "#         batch.add_data_object(properties, \"Article\")\n",
    "#         print(f\"Importing article: {index + 1}\")\n",
    "\n",
    "# for pre-vectorized data\n",
    "client.batch.configure(batch_size=100)\n",
    "with client.batch as batch:\n",
    "    for index, row in (combined_results.iterrows()):\n",
    "        properties = {\n",
    "            \"title\": row['title'],\n",
    "            \"publication_date\": row['publication_date'],\n",
    "            \"snippet\": row['snippet']\n",
    "        }\n",
    "        batch.add_data_object(properties, \"Article\", vector=row[\"snippet_vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphql (allows to group/merge similar results)\n",
    "# query_text = \"Find articles about police shooting that resulted in death of the victims. The incident is about police officer,deputy, sheriff, trooper, cop who fatally fired shots and killed someone. The victims are dead.\"\n",
    "query_text = \"Recent police shooting incidents leading to fatalities of the victims. The incidents are about police officer, deputy, sheriff, trooper, cop who fatally fired shots and killed someone.\"\n",
    "query_vector = model.encode(query_text).tolist()\n",
    "\n",
    "get_articles_group = f\"\"\"\n",
    "{{\n",
    "  Get {{\n",
    "    Article(\n",
    "      nearVector: {{\n",
    "        vector: {query_vector}\n",
    "      }},\n",
    "      group: {{\n",
    "        type: merge,\n",
    "        force: 0.15\n",
    "      }},\n",
    "      limit: 100\n",
    "    ) {{\n",
    "      title,\n",
    "      publication_date,\n",
    "      snippet,\n",
    "      _additional {{\n",
    "        generate(\n",
    "          singleResult: {{\n",
    "            prompt: \\\"\\\"\\\"Summarize the article: '{'{{snippet}}'}' in one sentence.\\\"\\\"\\\"\n",
    "          }}\n",
    "        ) {{\n",
    "          singleResult,\n",
    "          error\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "query_result = client.query.raw(get_articles_group)\n",
    "# save to csv\n",
    "df = pd.DataFrame(query_result['data']['Get']['Article'])\n",
    "df.to_csv(f'../data_storage/test_weaviate_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without grouping\n",
    "\n",
    "# nearText = {\"concepts\": [\"police shooting, shot by police, police shot, officer-involved shooting, police-involved shooting, police officer shooting, officer shot, deputy shot, sheriff shot, cop shot, trooper shot, shot by officer, shot by deputy, shot by sheriff, shot by cop, shot by trooper, killed by police, killed by officer, killed by deputy, killed by sheriff, killed by cop, killed by trooper\"]}\n",
    "# nearText = {\"concepts\": [\"a recent fatal police shooting. The incident is about police officer fired shots and killed someone. The victims are dead.\"]}\n",
    "# nearText = {\"concepts\": [\"Find articles about police shooting that resulted in death of the victims. The incident is about police officer fired shots and killed someone. The victims are dead.\"]}\n",
    "\n",
    "query_text = \"Find articles about police shooting that resulted in death of the victims. The incident is about police officer,deputy, sheriff, trooper, cop who fired shots and killed someone. The victims are dead. Example: Elk Grove Village police fatally shoot man they say was wielding knife Elk Grove police officers responded to reports of a man with a knife in the 200 block of Fern Drive, and once at the scene, a man exited a house with a knife, police said. He then confronted police in a nearby yard, and during the exchange, police shot him. Elk Grove police officers fatally shot a man whom they say was armed with a knife Friday night outside a home in the northwest suburb. Jack Murray, 24, was shot by officers about 4 p.m. in the 200 block of Fern Drive, according to police and the Cook County Medical Examiner√¢s Office. Officers were responding to reports of a man with a knife. Once officers arrived, Murray allegedly left a house with a knife and confronted police in a nearby yard, and during the exchange, police shot him. Murray was taken to a hospital where he was pronounced dead, police said. Three police officers were also taken to hospitals for observation. Afterward, police said there was no √¢active public safety risk.√¢ An attorney for Murray√¢s family said in a statement that they weren√¢t allowed back into their home late Friday, and that they had √¢no reason to believe that the anything inside the Murrays√¢ family home would be pertinent or relevant to any crime.√¢ √¢We hope to cooperate with the Elk Grove Village police to determine exactly what happened during this tragic incident,√¢ attorney Antonio Romanucci said. Elk Grove police are conducting a criminal investigation, and the regional Major Case Assistance Team is investigating the shooting.\"\n",
    "nearVector = {\"vector\": model.encode(query_text).tolist()}\n",
    "\n",
    "response = (\n",
    "    client.query\n",
    "    .get(\"Article\", [\"title\", \"publication_date\", \"snippet\"])\n",
    "    .with_near_vector(nearVector)  # Pass the nearVector_dict to with_near_vector\n",
    "    # .with_generate(single_prompt=\"Summarize {snippet} in one sentence.\")\n",
    "    .with_limit(50)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=4))\n",
    "# convert response to dataframe and save to csv\n",
    "df = pd.DataFrame(response['data']['Get']['Article'])\n",
    "df.to_csv(f'../data_storage/{start}_weaviate.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"38379429\"\n",
    "directory_api = mediacloud.api.DirectoryApi(auth_token='e85cce24da8b73eaa05329d258146c044ef055db')\n",
    "api = SearchApiClient(\"mediacloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the domain list into chunks\n",
    "def split_into_chunks(domains, chunk_size):\n",
    "    for i in range(0, len(domains), chunk_size):\n",
    "        yield domains[i:i + chunk_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sources: 8913\n"
     ]
    }
   ],
   "source": [
    "all_domains = []\n",
    "# Pagination setup\n",
    "offset = 0\n",
    "limit = 100  # seems to have a 100 limit\n",
    "more_pages = True\n",
    "\n",
    "while more_pages:\n",
    "    # Fetch sources with current offset\n",
    "    sources_response = directory_api.source_list(collection_id=collection_id, limit=limit, offset=offset)\n",
    "    sources = sources_response.get('results', [])\n",
    "    domains = [source['homepage'] for source in sources]\n",
    "    all_domains.extend(domains)\n",
    "    \n",
    "    # Update the offset\n",
    "    offset += limit\n",
    "    \n",
    "    # Check if there are more pages to fetch\n",
    "    more_pages = len(sources) == limit\n",
    "\n",
    "# Cleaning up domains\n",
    "cleaned_domains = [\n",
    "    domain.replace('https://www.', '').replace('http://www.', '').replace('https://','').replace('http://','').rstrip('/')\n",
    "    for domain in all_domains\n",
    "    if domain  # Ensure the domain is not None or empty\n",
    "]\n",
    "print(f\"Number of sources: {len(cleaned_domains)}\")\n",
    "\n",
    "domains_df = pd.DataFrame(cleaned_domains, columns=['Domain'])\n",
    "# Save the DataFrame to a CSV file\n",
    "domains_df.to_csv('domains.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_articles endpoint: 622 articles\n",
      "all_articles endpoint: 261 articles\n",
      "all_articles endpoint: 67 articles\n",
      "all_articles endpoint: 15 articles\n",
      "all_articles endpoint: 34 articles\n",
      "all_articles endpoint: 25 articles\n",
      "all_articles endpoint: 14 articles\n",
      "all_articles endpoint: 14 articles\n",
      "all_articles endpoint: 2 articles\n",
      "                                                 title publication_date  \\\n",
      "256  Antioch: Man wanted for homicide is shot by po...       2023-10-01   \n",
      "112  Antioch: Man wanted for homicide is shot by po...       2023-10-01   \n",
      "328  1 killed in police shooting at Pa. traffic sto...       2023-10-01   \n",
      "417  St. Louis police shoot man who threatened rela...       2023-10-01   \n",
      "173  Demonstrators call for justice after deadly po...       2023-10-01   \n",
      "\n",
      "             capture_time language            domain  \\\n",
      "256  2023-10-03T11:47:16Z       en   mercurynews.com   \n",
      "112  2023-10-03T13:10:17Z       en  eastbaytimes.com   \n",
      "328  2023-10-13T01:29:15Z       en      pennlive.com   \n",
      "417  2023-10-08T19:59:04Z       en      stltoday.com   \n",
      "173  2023-10-03T04:00:43Z       en          ktla.com   \n",
      "\n",
      "                                                   url  \\\n",
      "256  https://www.mercurynews.com/2023/10/01/antioch...   \n",
      "112  https://www.eastbaytimes.com/2023/10/01/antioc...   \n",
      "328  https://www.pennlive.com/news/2023/10/1-killed...   \n",
      "417  https://www.stltoday.com/news/local/crime-cour...   \n",
      "173  https://ktla.com/news/local-news/demonstrators...   \n",
      "\n",
      "                                  original_capture_url  \\\n",
      "256  https://web.archive.org/web/20231003114716id_/...   \n",
      "112  https://web.archive.org/web/20231003131017id_/...   \n",
      "328  https://web.archive.org/web/20231013012915id_/...   \n",
      "417  https://web.archive.org/web/20231008195904id_/...   \n",
      "173  https://web.archive.org/web/20231003040043id_/...   \n",
      "\n",
      "                                  archive_playback_url  \\\n",
      "256  https://web.archive.org/web/20231003114716/htt...   \n",
      "112  https://web.archive.org/web/20231003131017/htt...   \n",
      "328  https://web.archive.org/web/20231013012915/htt...   \n",
      "417  https://web.archive.org/web/20231008195904/htt...   \n",
      "173  https://web.archive.org/web/20231003040043/htt...   \n",
      "\n",
      "                                           article_url  \n",
      "256  https://wayback-api.archive.org/colsearch/v1/m...  \n",
      "112  https://wayback-api.archive.org/colsearch/v1/m...  \n",
      "328  https://wayback-api.archive.org/colsearch/v1/m...  \n",
      "417  https://wayback-api.archive.org/colsearch/v1/m...  \n",
      "173  https://wayback-api.archive.org/colsearch/v1/m...  \n"
     ]
    }
   ],
   "source": [
    "domain_chunks = list(split_into_chunks(cleaned_domains, 1000))\n",
    "\n",
    "# sources_response = directory_api.source_list(collection_id=collection_id)\n",
    "# domains = [source['homepage'] for source in sources_response['results']]\n",
    "# num_sources = len(sources_response['results'])\n",
    "# print(f\"Number of sources: {num_sources}\")\n",
    "\n",
    "# cleaned_domains = [domain.replace('https://www.', '').replace('http://www.', '').replace('https://','').replace('http://','') for domain in domains]\n",
    "# cleaned_domains = [url.rstrip('/') for url in cleaned_domains]\n",
    "# print(cleaned_domains)\n",
    "\n",
    "# Enable requests cache\n",
    "requests_cache.install_cache('article_cache', backend='filesystem', expire_after=3600)\n",
    "\n",
    "\n",
    "# Query parameters\n",
    "query_term = '(\"police shooting\" OR \"shot by police\" OR \"police shot\" OR \"officer-involved shooting\" OR \"police-involved shooting\" OR \"police officer shooting\" OR \"police shot\" OR \"officer shot\")'\n",
    "# query_term = '(\"police shooting\" OR \"shot by police\" OR \"police shot\" OR \"officer-involved shooting\" OR \"police-involved shooting\" OR \"police officer shooting\" OR \"police shot\" OR \"officer shot\" OR \"deputy shot\" OR \"sheriff shot\" OR \"cop shot\")'\n",
    "# query_term = 'police AND shot'\n",
    "start = datetime(2023, 9, 1) #11/6 - 11/15\n",
    "end = datetime(2023, 10, 1) \n",
    "language = \"en\"\n",
    "\n",
    "# DataFrame to store combined results\n",
    "combined_results = pd.DataFrame()\n",
    "results_list = []\n",
    "\n",
    "for chunk in domain_chunks:\n",
    "    domains_str = f\"domain:({' OR '.join(chunk)})\"\n",
    "    query = f\"{query_term} AND language:{language} AND {domains_str}\"\n",
    "    \n",
    "    # Perform the search with the current chunk\n",
    "    articles = []\n",
    "    for list_of_articles in api.all_articles(query, start, end):\n",
    "        articles.extend(list_of_articles)\n",
    "        print(f\"all_articles endpoint: {len(articles)} articles\")\n",
    "    \n",
    "    if articles:\n",
    "        chunk_results = pd.DataFrame(articles)\n",
    "        results_list.append(chunk_results)\n",
    "    \n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_results = pd.concat(results_list, ignore_index=True)\n",
    "combined_results.sort_values(by='publication_date', ascending=False, inplace=True)\n",
    "print(combined_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippet_from_newspaper3k(url):\n",
    "    extracted_article = Article(url)\n",
    "    extracted_article.download()\n",
    "    extracted_article.parse()\n",
    "    return extracted_article.text\n",
    "\n",
    "def get_snippet_from_wayback_machine(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    snippet = response.json().get('snippet', '')\n",
    "    # return response.text \n",
    "    return snippet\n",
    "\n",
    "def fetch_snippet(article_url, wayback_url):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        future_to_method = {\n",
    "            executor.submit(get_snippet_from_newspaper3k, article_url): 'newspaper3k',\n",
    "            executor.submit(get_snippet_from_wayback_machine, wayback_url): 'wayback'\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_method):\n",
    "            method = future_to_method[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                return data, method\n",
    "            except Exception as exc:\n",
    "                print(f\"{method} method failed with exception: {exc}\")\n",
    "\n",
    "\n",
    "def sanitize_snippet(snippet):\n",
    "    sanitized_snippet = snippet.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "    # Normalize Unicode characters\n",
    "    # normalized_snippet = unicodedata.normalize('NFKD', sanitized_snippet)\n",
    "    return sanitized_snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, article in tqdm(combined_results.iterrows(), total=combined_results.shape[0]):\n",
    "    article_url = article['url']\n",
    "    wayback_url = article['article_url']\n",
    "    snippet, method_used = fetch_snippet(article_url, wayback_url)\n",
    "    if snippet:\n",
    "        sanitized_snippet = sanitize_snippet(snippet)\n",
    "        combined_results.loc[index, 'snippet'] = sanitized_snippet\n",
    "        print(f\"Snippet fetched using {method_used} method for URL: {article_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results.to_csv(f'./data_storage/{start}.csv', index=False)\n",
    "print(f\"Data retrieval complete. Results saved to './data_storage/{start}.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in /Users/karenwang/.virtualenvs/semantic-search/lib/python3.11/site-packages (6.0.10)\n",
      "Requirement already satisfied: pandas in /Users/karenwang/.virtualenvs/semantic-search/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: sgmllib3k in /Users/karenwang/.virtualenvs/semantic-search/lib/python3.11/site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/karenwang/.virtualenvs/semantic-search/lib/python3.11/site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/karenwang/.virtualenvs/semantic-search/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/karenwang/.virtualenvs/semantic-search/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/karenwang/.virtualenvs/semantic-search/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/karenwang/.virtualenvs/semantic-search/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
